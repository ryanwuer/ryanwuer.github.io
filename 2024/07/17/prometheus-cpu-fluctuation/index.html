<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Prometheus CPU利用率震荡问题分析 | iamyeka</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="分析 Rate 函数计算逻辑，解释单核 Idle Rate &gt; 1 的背后原因">
<meta property="og:type" content="article">
<meta property="og:title" content="Prometheus CPU利用率震荡问题分析">
<meta property="og:url" content="http://yoursite.com/2024/07/17/prometheus-cpu-fluctuation/index.html">
<meta property="og:site_name" content="iamyeka">
<meta property="og:description" content="分析 Rate 函数计算逻辑，解释单核 Idle Rate &gt; 1 的背后原因">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/gallery/wufazhuce-1030.png">
<meta property="article:published_time" content="2024-07-16T16:00:00.000Z">
<meta property="article:modified_time" content="2025-03-11T13:44:50.048Z">
<meta property="article:author" content="iamyeka">
<meta property="article:tag" content="prometheus">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/gallery/wufazhuce-1030.png">
  
    <link rel="icon" href="https://blog-1302533045.cos.ap-nanjing.myqcloud.com/images/icon.001.png">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/highlightjs-line-numbers.js@2.6.0/dist/highlightjs-line-numbers.min.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', (event) => {
      document.querySelectorAll('pre code').forEach((block) => {
        hljs.highlightBlock(block);``
        hljs.lineNumbersBlock(block);
      });
    });
  </script>
  
<link rel="stylesheet" href="/css/index.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body style="


    background-color: #eff0f6

        ">
<div id="container">
    <nav id="nav">
  <header class="header">
    <a href="/" class="title"></a>
  </header>
  <div class="ctnWrap">
    <div class="icons">
      
        
          
            <a href="https://github.com/iamyeka" target="_blank" class="nav-icn iconfont icon-github">·GitHub</a>
          
        
      
    </div>
    <div class="menu">
      
        
            <a href="/" class="nav-menu ">主页</a>
          
        
            <a href="/archives" class="nav-menu ">归档</a>
          
        
            <a href="/tag" class="nav-menu ">标签</a>
          
        
            <a href="/about" class="nav-menu ">关于</a>
          
        
      
    </div>
  </div>
</nav>
    <div id="main"><section class="article">
  <h2 class="title">Prometheus CPU利用率震荡问题分析</h2>
  <p class="sub">2024-07-17</p>
  <article class="content">
    <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>通过grafana发现，K8S集群节点利用率存在突刺的情况，具体如下图所示：</p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/resource-fluctuation.png" alt="资源突刺"></p>
<p>现象：</p>
<ul>
<li>波动很大，有突然下落到很低利用率的情况</li>
</ul>
<h2 id="监控数据"><a href="#监控数据" class="headerlink" title="监控数据"></a>监控数据</h2><h3 id="表达式说明"><a href="#表达式说明" class="headerlink" title="表达式说明"></a>表达式说明</h3><blockquote>
<p>CPU 利用率 看板计算表达式</p>
</blockquote>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/node-panel-expression.png" alt="节点利用率看板表达式"></p>
<p>含义：<strong>查出每个节点上的单核平均利用率，做平均得到整个K8s集群的单核平均使用率</strong></p>
<blockquote>
<p> instance:node_cpu_utilisation:rate1m</p>
</blockquote>
<pre><code class="yaml">- expr: |-
    1 - avg without (cpu, mode) (
      rate(node_cpu_seconds_total{job=&quot;node-exporter&quot;, mode=&quot;idle&quot;}[1m])
    )
  record: instance:node_cpu_utilisation:rate1m</code></pre>
<p>含义：<strong>每个节点上的单核平均利用率（最近1分钟的rate）</strong></p>
<p>node_cpu_seconds_total</p>
<p>含义：用于度量节点 CPU 使用情况。这个指标提供了 CPU 在不同模式下的使用时间，以秒为单位。</p>
<p>这个指标是一个 Counter 类型的指标，这意味着它的值只会增加（除非在系统重启时重置）。你可以通过计算这个指标在一段时间内的增量来度量 CPU 的使用率。</p>
<pre><code class="yaml">node_cpu_seconds_total{container=&quot;node-exporter&quot;,cpu=&quot;0&quot;,endpoint=&quot;metrics&quot;,instance=&quot;10.239.83.75:9100&quot;,job=&quot;node-exporter&quot;,mode=&quot;idle&quot;,namespace=&quot;monitoring&quot;,pod=&quot;kube-prometheus-prometheus-node-exporter-8775m&quot;,service=&quot;kube-prometheus-prometheus-node-exporter&quot;}    </code></pre>
<ul>
<li>cpu：每个核心的标号对应一个取值。如10.239.83.75这台机器是96核机器，那么cpu就有0～95个取值</li>
<li>mode：cpu 运行模式<ul>
<li><code>idle</code>: CPU 处于空闲状态的时间。</li>
<li><code>user</code>: 用户级别应用程序的 CPU 使用时间。这包括大部分应用程序和进程。</li>
<li><code>system</code>: 内核级别应用程序的 CPU 使用时间。这包括处理系统调用和内核任务的时间。</li>
<li><code>iowait</code>: CPU 等待 I/O 操作完成的时间。</li>
<li><code>nice</code>: 改变优先级的用户级别应用程序的 CPU 使用时间。</li>
<li><code>irq</code>: 处理硬件中断的 CPU 使用时间。</li>
<li><code>softirq</code>: 处理软件中断的 CPU 使用时间。</li>
<li><code>steal</code>: 在虚拟环境中，等待虚拟 CPU 而被其他虚拟机占用的时间。</li>
</ul>
</li>
</ul>
<p>综上，节点上单核平均利用率计算分成3步：</p>
<ol>
<li>计算节点上 每个CPU核心的 idle mode 最近一分钟内的平均cpu使用</li>
</ol>
<pre><code>rate(node_cpu_seconds_total{job=&quot;node-exporter&quot;, mode=&quot;idle&quot;}[1m]</code></pre><ol start="2">
<li>计算节点维度，所有核心的平均 idle mode 使用</li>
</ol>
<pre><code>avg without (cpu, mode) (
  rate(node_cpu_seconds_total{job=&quot;node-exporter&quot;, mode=&quot;idle&quot;}[1m])
)</code></pre><ol start="3">
<li>计算节点维度，所有核心的平均实际使用</li>
</ol>
<pre><code>1 - avg without (cpu, mode) (
  rate(node_cpu_seconds_total{job=&quot;node-exporter&quot;, mode=&quot;idle&quot;}[1m])
)</code></pre><h3 id="Rate、IRate、Increase"><a href="#Rate、IRate、Increase" class="headerlink" title="Rate、IRate、Increase"></a>Rate、IRate、Increase</h3><p>如下图：假设现在是9点30分，我们每隔 5s 采样一次，在 09:30:23 查询最近 20s 的 rate 和 irate 值，也就是 [3s, 23s] 的区间内增长了多少？这里的问题在于查询区间的时间与采样时间不重合</p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/rate-calculate.png" alt="rate如何计算"></p>
<p><strong>Rate</strong></p>
<p>取 20s 内最近和最远的两个采样点： {5s: 10} 、{20s: 30}，并计算它们的区间为 20 - 5 = 15s，期间请求量增长了 30 - 10 = 20 次。因此 rate(xxx[20s]) = 20 / 15 = 1.3333333</p>
<p>问题：Promtheus 如何解决exporter重启导致Counter值被重置？（counter下降，就认为是一种重置）</p>
<p>例如60秒内有下面6数值，在第四个数字后面发生了重置</p>
<pre><code>2 4 6 8 2 4</code></pre><p>相关代码如下：</p>
<pre><code class="go">var (
    counterCorrection float64 // 修正值
    lastValue         float64 // 最后一个值
)
for _, sample := range samples.Points {
    // 每次出现counter值重置的情况，修正值就加上重置前的值
    if isCounter &amp;&amp; sample.V &lt; lastValue {
       counterCorrection += lastValue
    }
    lastValue = sample.V // 更新最后一个值
}
// 最后一个值 - 第一个值 + 修正值
resultValue := lastValue - samples.Points[0].V + counterCorrection</code></pre>
<p>2小于lastValue 8，所以 counterCorrection = 8</p>
<p>最后的 resultValue = 4 - 2 + 8 = 10，当然，重置的情况很少，这里如果不重置数据，假设Counter线性增长， [ 2 4 6 8 10 12 ]，就是最后一个值减去第一个值resultValue = 12 - 2 + 0 和 重置算得一样</p>
<p><strong>IRate</strong></p>
<p>取 20s 内最近的两个采样点：{15s: 20} 、{20s: 30}，并计算它们的区间为 20 - 15 = 5s，期间请求量增长了 30 - 20 = 10 次。因此 irate(xxx[20s]) = 10 / 5 = 2</p>
<p><strong>Increase</strong></p>
<p>做线性外插（等比延伸），如上图右侧展示，会计算出rate值后乘上increase的时间区间</p>
<p>Increase =  rate  <em>* 20 =</em> 20 / 15 * 20 =  26.67</p>
<p>这也解释了为什么int类型的Counter值，通过increase计算出来的值不是int</p>
<h3 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h3><p>查看 <code>instance:node_cpu_utilisation:rate1m</code>，也就是 <code>1 - avg without (cpu, mode) (rate(node_cpu_seconds_total{job=&quot;node-exporter&quot;, mode=&quot;idle&quot;}[1m]))</code></p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/node-rate-negative.png" alt="node利用率-负数"></p>
<p>出现某些节点的值是<font color='red'>负数</font>的情况，且达到很大的值：0.5，会间歇性发生</p>
<p>可想而知，在这些节点的利用率有负数情况出现时，在计算平均节点单核利用率时，将大幅度拉低整体的值，导致出现断崖式下滑的情况</p>
<p>单独看某个节点 idle cpu 的rate 1m的值，有某些核心的值 &gt; 1的情况</p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/idle-cpu-rate-over-one.png" alt="某些核心idle大于1"></p>
<p><strong>为什么 rate  idle 会大于1 ？</strong></p>
<p><strong>猜想</strong></p>
<p>问题要么出在Node-exporter，要么出在Prometheus</p>
<blockquote>
<p>排查 Node-exporter</p>
</blockquote>
<ul>
<li>尝试 将 Node-Exporter（daemonset服务，挂载主机/proc目录，获取监控信息）升级到最新版本 —— 未能解决问题</li>
<li>尝试编写Client 每隔30s 拉取一次Node-Exporter指标，看相邻的两次样本值的差值</li>
</ul>
<p>日志打印如下，一共运行了20min，只有一次样本差值 30.019999995827675 微微超过30，考虑到样本值的类型是一个float，并且定时任务也无法保证间隔是精确的30s，所以这次可以认为在误差允许范围内</p>
<p><font color='green'>基本可以判断 Node-exporter的逻辑没有问题</font></p>
<pre><code>2024-04-19 16:11:35.723807 +0800 CST m=+60.001623334
Gap: 29.849999994039536

2024-04-19 16:12:05.724161 +0800 CST m=+90.002162126
Gap: 29.87000000476837

2024-04-19 16:12:35.723935 +0800 CST m=+120.002120709
Gap: 29.53999999910593

2024-04-19 16:13:05.722918 +0800 CST m=+150.001288043
Gap: 27.87999999523163

2024-04-19 16:13:35.738429 +0800 CST m=+180.016983334
Gap: 29.890000000596046

2024-04-19 16:14:05.754946 +0800 CST m=+210.013578418
Gap: 29.87000000476837

2024-04-19 16:14:35.752635 +0800 CST m=+240.002515584
Gap: 30.019999995827675

2024-04-19 16:15:05.753538 +0800 CST m=+270.002130959
Gap: 29.80000000447035

...</code></pre><blockquote>
<p>排查 Prometheus</p>
</blockquote>
<ul>
<li>可能和负载压力有关系。Prometheus cpu使用达到 <strong>15.5核心</strong>，出现 <strong>23.4%</strong>的被限流的情况（30,000+的协程处理各项任务（指标拉取、聚合rule计算、告警规则计算、服务发现、配置热加载等等）</li>
</ul>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/prometheus-throttle.png" alt="prometheus-throttle"></p>
<p><strong>观测：</strong></p>
<p>基于上述的 rate 计算逻辑，我们先取出 Prometheus 原始采集的样本数据，选择某个idle rate &gt; 1的时间点，找最近1分钟的采样点，一共有2个（30s的采集周期，符合预期）：</p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/image-20240425120354924.png" alt="image-20240425120354924"></p>
<ol>
<li>采样1: 88647709.3 @1713341448.972</li>
<li>采样2: 88647751.24 @1713341479.372</li>
</ol>
<p>样本值差值：88647751.24 - 88647709.3 = 41.94</p>
<p>时间戳差值：1713341479.372 - 1713341448.972 = 30.4000001</p>
<p>rate =  41.94 /  30.4000001 = 1.37960526 &gt; 1   </p>
<p>与Prometheus 界面上通过Rate函数计算出的一致，验证了Rate计算的逻辑</p>
<p>取其他时刻点，发现时间戳的值普遍 &gt; 30s，有的达到31s、32s，明明配置了30s的采集间隔，为什么会有这种差异？</p>
<h4 id="为什么时间戳差值-gt-30s？"><a href="#为什么时间戳差值-gt-30s？" class="headerlink" title="为什么时间戳差值 &gt; 30s？"></a>为什么时间戳差值 &gt; 30s？</h4><p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/node-exporter-honor-timestamp.png" alt="node-exporter-尊重时间戳"></p>
<p>Node-exporter 采集任务配置了 <strong>尊重指标来源设置的时间戳</strong>（Prometheus 默认采用此配置），但从指标接口拉取的指标里不包含时间戳，所以最终会使用Prometheus自身的时间戳，作为指标最终的时间戳</p>
<p>包含时间戳的指标返回格式样例：</p>
<pre><code class="yaml">container_health_check_duration_millisecond{container_name=&quot;prometheus-node-exporter&quot;,namespace=&quot;monitoring&quot;,pod_name=&quot;kube-prometheus-prometheus-node-exporter-znxsv&quot;} 2.3509091e+07 1713756155090 </code></pre>
<ul>
<li><strong>样本名</strong>：container_health_check_duration_millisecond （内在实现上，存储为 “ __name_&quot;_ = “container_health_check_duration_millisecond” 的标签对）</li>
<li><strong>样本标签对</strong>：{container_name=”prometheus-node-exporter”,namespace=”monitoring”,pod_name=”kube-prometheus-prometheus-node-exporter-znxsv”} </li>
<li><strong>样本值</strong>：2.3509091e+07</li>
<li><strong>样本时间戳</strong>：1713756155090  -&gt; 由客户端实现决定，node-exporter没有实现这一逻辑（绝大多数Exporter，都没有这样实现，原因待分析），接口返回里就没有这个字段</li>
</ul>
<pre><code class="go">// 客户端调用此方法可以给指标带上时间戳
ch &lt;- prometheus.NewMetricWithTimestamp(time.Now(), metric)</code></pre>
<p><strong>Prometheus指标时间戳设置流程</strong></p>
<p>Target：指一个拉取指标的目标地址，如 <a href="http://1.2.3.4:8080/metrics" target="_blank" rel="noopener">http://1.2.3.4:8080/metrics</a></p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/prom-set-timestamp.png" alt="Prometheus设置时间戳"></p>
<p>分析：</p>
<p>Prometheus每个副本启动了30,000+的协程处理各项任务（指标拉取、聚合rule计算、告警规则计算、服务发现、配置热加载等等）协程间的切换频繁，会导致：</p>
<ul>
<li>30s一次的 <strong>时间间隔 无法得到保证</strong>，如上面的例子里就达到了 30.4000001s</li>
<li>定时任务触发后，在Http请求发出前，可能因<strong>协程切换，导致请求未得到有效处理，即实际等待了一段时间，才发出请求</strong>。因 <strong>协程间的切换的不确定性</strong>，有可能前一次拉取没有上面这种情况，但第二次拉取出现了，最终会导致两次Count的值的差值偏大，超过了两次拉取的时间差值，就会导致最终相除 &gt; 1</li>
</ul>
<p>示意图如下，在10:31:00，计算rate 1m的值，区间内有两个点：</p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/difference-between-2-goroutine.png" alt="两次拉取协程切换导致的差异"></p>
<p><strong>验证负载压力的影响：</strong></p>
<p>在测试环境的 Prometheus中（负载压力很低），配置静态拉取，选一台节点作为目标</p>
<p>果然，差异出现了。测试环境的Rate计算始终不会超过1</p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/test-prom-rate.png" alt="测试环境rate值"></p>
<p>可以实锤和 Prometheus 的负载压力有关</p>
<h1 id="改进措施"><a href="#改进措施" class="headerlink" title="改进措施"></a>改进措施</h1><h2 id="措施1：去掉-CPU-limit"><a href="#措施1：去掉-CPU-limit" class="headerlink" title="措施1：去掉 CPU limit"></a>措施1：去掉 CPU limit</h2><p>作用：解决Prometheus限流问题 —— 去掉 CPU limit</p>
<p>效果：最近一天的单核心平均CPU利用率</p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/image-20240425135651030.png" alt="image-20240425135651030"></p>
<p>总结：</p>
<ol>
<li>一下子下落很多的情况基本消失了</li>
<li>整体上下波动还是比较大</li>
</ol>
<h2 id="措施2：调大Rate时间区间"><a href="#措施2：调大Rate时间区间" class="headerlink" title="措施2：调大Rate时间区间"></a>措施2：调大Rate时间区间</h2><p>作用：减轻Prometheus负载高导致的协程任务无法及时处理的影响</p>
<p>首先看下社区最新是如何统计节点利用率的</p>
<p><strong>prometheus-community 社区实践</strong>（我们的Prometheus Chart基于他们的3年前的版本）</p>
<p><a href="https://github.com/prometheus-community/helm-charts/blob/71f21809fe80b57368e1bbf2a938e8fdc1322002/charts/kube-prometheus-stack/templates/prometheus/rules-1.14/node-exporter.rules.yaml#L42" target="_blank" rel="noopener">https://github.com/prometheus-community/helm-charts/blob/71f21809fe80b57368e1bbf2a938e8fdc1322002/charts/kube-prometheus-stack/templates/prometheus/rules-1.14/node-exporter.rules.yaml#L42</a></p>
<pre><code class="yaml">- expr: |-
    1 - avg without (cpu) (
      sum without (mode) (rate(node_cpu_seconds_total{job=&quot;node-exporter&quot;, mode=~&quot;idle|iowait|steal&quot;}[5m]))
    )
  record: instance:node_cpu_utilisation:rate5m</code></pre>
<p>两处差异：</p>
<ol>
<li>不仅仅将idle，还将iowait和steal的cpu使用反选</li>
<li>rate 时间为5m</li>
</ol>
<p><strong>另一处社区实践：</strong></p>
<p><a href="https://monitoring.mixins.dev/node-exporter/" target="_blank" rel="noopener">https://monitoring.mixins.dev/node-exporter/</a></p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/monitoring-mixins-node-rate.png" alt="monitoring-mixins-node利用率"></p>
<p>和 prometheus-community 社区的实践一致</p>
<p>我们当前使用的Rate时间区间是1m，和拉取间隔30s是不匹配的。建议 Rate的区间至少设置为拉取间隔的4倍，在如下文章里有提到：</p>
<p><a href="https://yasongxu.gitbook.io/container-monitor/yi-.-kai-yuan-fang-an/di-2-zhang-prometheus/prometheus-use#rate-de-ji-suan-luo-ji" target="_blank" rel="noopener">https://yasongxu.gitbook.io/container-monitor/yi-.-kai-yuan-fang-an/di-2-zhang-prometheus/prometheus-use#rate-de-ji-suan-luo-ji</a></p>
<p>“建议将rate计算的范围向量的时间至少设为抓取间隔的四倍。这将确保即使抓取速度缓慢，且发生了一次抓取故障，您也始终可以使用两个样本。此类问题在实践中经常出现，因此保持这种弹性非常重要。例如，对于1分钟的抓取间隔，您可以使用4分钟的rate 计算，但是通常将其四舍五入为5分钟。”</p>
<p>将rate的时间区间调整为<strong>5m</strong>，再次查询。节点cpu利用率出现负数的情况大大减少，值也很小，几乎消失了</p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/rate5m.png" alt="rate5m"></p>
<p>总结：</p>
<ul>
<li><p>当把rate区间从1m调整为5m，虽无法彻底规避协程切换带来的影响 —— 相邻拉取任务 指标值的差异不对应时间戳的差异，但在更长时间范围内进行统计，这种差异可以缩小，尤其在计算比例时。</p>
<p>rate = 指标值差异 / 时间戳差异</p>
<p>覆盖的采集点从 2个 -&gt; 10个，有的相邻采样点的差异趋于一致，会使得整体差异变小</p>
</li>
<li><p>另外，rate 5m也可以使得整体利用率曲线更加平稳，不会出现短时间内一直来回波动的情况</p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/image-20240425150139036.png" alt="image-20240425150139036"></p>
</li>
</ul>
<h1 id="文档参考"><a href="#文档参考" class="headerlink" title="文档参考"></a>文档参考</h1><ul>
<li><a href="https://lotabout.me/2019/QQA-Why-Prometheus-increase-return-float/" target="_blank" rel="noopener">为什么 Prometheus increase 不返回整数？</a></li>
<li><a href="https://zhangguanzhang.github.io/2020/07/30/prometheus-rate-and-irate/" target="_blank" rel="noopener">prometheus的rate与irate内部是如何计算的</a></li>
<li><a href="https://www.youtube.com/watch?v=67Ulrq6DxwA" target="_blank" rel="noopener">Promethues 如何设计 Counter</a> Prometheus团队在KubeCon的分享，非常经典，推荐观看</li>
<li><a href="https://yasongxu.gitbook.io/container-monitor/yi-.-kai-yuan-fang-an/di-2-zhang-prometheus/prometheus-use" target="_blank" rel="noopener">高可用prometheus：常见问题</a> 作者做了大量的调研工作，解决了监控领域大量的困惑，推荐阅读</li>
<li><a href="https://www.robustperception.io/blog/" target="_blank" rel="noopener">Reliable Insights — A blog on monitoring, scale and operational Sanity</a> Prometheus核心开发者的博客，很多干货</li>
</ul>

  </article>
  <footer class="f-cf">
    
      <a href="/2024/07/26/codemagic/" class="link f-fl">⟵codemagic 调研</a>
    
    
      <a href="/2022/06/24/push-image-to-harbor/" class="link f-fr">镜像如何推送到Harbor⟶</a>
    
  </footer>
</section>


    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<div id="gitalk-container"></div>
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: '4e08eb89d32a7fa72ccd',
        clientSecret: 'a3d809fa86c720cc9b944a53e689ad1e9779b162',
        id: location.pathname,
        repo: 'iamyeka.github.io',
        owner: 'iamyeka',
        admin: ['iamyeka']
    })
    gitalk.render('gitalk-container')
</script>
</div>
    <footer id="footer" class="f-cf">
</footer>
</div>
<a href="https://github.com/iamyeka" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub">
    <svg width="80" height="80" viewBox="0 0 250 250"
         style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
<style>.github-corner:hover .octo-arm {
        animation: octocat-wave 560ms ease-in-out
    }

    @keyframes octocat-wave {
        0%, 100% {
            transform: rotate(0)
        }
        20%, 60% {
            transform: rotate(-25deg)
        }
        40%, 80% {
            transform: rotate(10deg)
        }
    }

    @media (max-width: 500px) {
        .github-corner:hover .octo-arm {
            animation: none
        }

        .github-corner .octo-arm {
            animation: octocat-wave 560ms ease-in-out
        }
    }</style>
</body>
</html>
