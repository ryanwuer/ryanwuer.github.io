<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Prometheus 生产问题记录 | ryanwuer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="记录笔者在 2020-2025 年期间遇到的一些问题和解决手段">
<meta property="og:type" content="article">
<meta property="og:title" content="Prometheus 生产问题记录">
<meta property="og:url" content="http://yoursite.com/2025/04/07/prometheus-production-issues/index.html">
<meta property="og:site_name" content="ryanwuer">
<meta property="og:description" content="记录笔者在 2020-2025 年期间遇到的一些问题和解决手段">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/gallery/wufazhuce-1035.png">
<meta property="article:published_time" content="2025-04-06T16:00:00.000Z">
<meta property="article:modified_time" content="2025-04-10T09:18:08.675Z">
<meta property="article:author" content="ryanwuer">
<meta property="article:tag" content="云原生">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/gallery/wufazhuce-1035.png">
  
    <link rel="icon" href="https://blog-1302533045.cos.ap-nanjing.myqcloud.com/images/icon.001.png">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/highlightjs-line-numbers.js@2.6.0/dist/highlightjs-line-numbers.min.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', (event) => {
      document.querySelectorAll('pre code').forEach((block) => {
        hljs.highlightBlock(block);``
        hljs.lineNumbersBlock(block);
      });
    });
  </script>
  
<link rel="stylesheet" href="/css/index.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body style="


    background-color: #eff0f6

        ">
<div id="container">
    <nav id="nav">
  <header class="header">
    <a href="/" class="title"></a>
  </header>
  <div class="ctnWrap">
    <div class="icons">
      
        
          
            <a href="https://github.com/ryanwuer" target="_blank" class="nav-icn iconfont icon-github">·GitHub</a>
          
        
      
    </div>
    <div class="menu">
      
        
            <a href="/" class="nav-menu ">主页</a>
          
        
            <a href="/archives" class="nav-menu ">归档</a>
          
        
            <a href="/tag" class="nav-menu ">标签</a>
          
        
            <a href="/about" class="nav-menu ">关于</a>
          
        
      
    </div>
  </div>
</nav>
    <div id="main"><section class="article">
  <h2 class="title">Prometheus 生产问题记录</h2>
  <p class="sub">2025-04-07</p>
  <article class="content">
    <h1 id="问题一：Prometheus-内存持续上升，最终OOM"><a href="#问题一：Prometheus-内存持续上升，最终OOM" class="headerlink" title="问题一：Prometheus 内存持续上升，最终OOM"></a>问题一：Prometheus 内存持续上升，最终OOM</h1><p>版本：v2.22.2<br>变更事项：Prometheus磁盘升级 1.6T → 6.5T、保留时间 20d → 90d<br>保留时间会影响另一项参数：storage.tsdb.max-block-duration，当这个参数不显示配置时，默认等于保留时间的10%，也就是说其值从2d → 9d<br>直接导致Prometheus需要另外启动协程，将2d的block进一步合并为更大的block，需要消耗更多的CPU，有可能规定时间内无法执行完，指标量还一直在加，导致后续的任务也无法完成</p>
<p>副本1 走变更，副本 0 回退，观测Compaction次数对比<br><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/POPO-20250407-145707.png" alt="img"><br>结论：</p>
<ol>
<li>副本0的Compaction次数符合预期</li>
<li>副本1的Compaction次数停在了1，说明Compaction的任务一直没有完结</li>
</ol>
<p>关键日志：<br>如下图block时间范围：[2023-01-21 02:00:00, 2023-01-22 14:00:00]，间隔1.5d，压缩花费52m</p>
<pre><code class="textmate">level=info ts=2023-02-15T04:57:02.036Z caller=compact.go:440 component=tsdb msg=&quot;compact blocks&quot; count=2 mint=1674237600000 maxt=1674367200000 ulid=01GS9KXZ9ZH7QBKJ0YGMWCKQVP sources=&quot;[01GQAFN2DXA7TYZ4BW91YWX9S0 01GQCDH4A1EDNV69RKBVHSPS5G]&quot; duration=52m19.604933448s</code></pre>
<p>Compact 是取3个同样大小的block进行合并，所以下一个更大的block的时间区间将达到6d18h，花费时间可能会突破3h，那对整体的任务肯定会造成影响</p>
<p>临时解决方案：</p>
<ol>
<li>配置 storage.tsdb.max-block-duration = 2d，锁定住最大的block时间范围</li>
</ol>
<p><strong>接下来排查为什么任务无法完结，CPU耗费在了哪里？</strong><br>通过pprof，看 Compact 协程的运行情况<br><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/POPO-20250410-171410.png" alt="img"><br>分析：运行了1s，但等待调度的时间达到了6s，抢不到CPU！接下来分析CPU被什么模块占用了</p>
<p><img src="https://iamyeka.oss-cn-hangzhou.aliyuncs.com/blog/images/POPO-20250407-150410.png" alt="img"><br>有一块 scrape.reload 的CPU占比较高，这块和服务发现有关系，查看代码得知，我们用的Prometheus版本，会固定每 5s 做一次targets列表的刷新<br>社区对该参数做了可配置的修改：<a href="https://github.com/prometheus/prometheus/commit/41630b8e8835585098f345b0d9740d7107ffb6eb" target="_blank" rel="noopener">https://github.com/prometheus/prometheus/commit/41630b8e8835585098f345b0d9740d7107ffb6eb</a><br>在 v2.36.0 版本里发布上线。discovery-reload-interval flag修改为2m后，CPU使用大大降低，放开 max-block-duration 到 9d，Compaction的任务也能正常完成了</p>
<p><strong>继续深究下，为什么reload target这么耗费cpu呢？</strong><br>target对象里会包含目标的一些元数据，包括namespace、labels、annotations等，这些会参与排序、唯一性判断等操作，很消耗cpu<br>看下我们下发的一个Pod：</p>
<pre><code class="yaml">metadata:
  name: xxx
  generateName: xxx
  namespace: xxx
  annotations:
    cloudnative.com/http-probe.sh: |
      http_probe(){
        local FUNC=&quot;$1&quot; URL=&quot;$2&quot; RETRY=&quot;$3&quot; SLEEP=&quot;$4&quot; \
              TIMEOUT=&quot;$5&quot; SLEEP_WHEN_SUCCEED=&quot;$6&quot;  BASEDIR=&quot;$7&quot; \
              PROBE_RESULT_DIR=&quot;${PROBE_RESULT_DIR:-&quot;.probe-result&quot;}&quot;
        local RESP_BODY_FILE=&quot;$BASEDIR/$PROBE_RESULT_DIR/$FUNC.res&quot; ERROR_FILE=&quot;$BASEDIR/$PROBE_RESULT_DIR/$FUNC.err&quot; &amp;&amp; \
             { [[ ! -d &quot;$BASEDIR/$PROBE_RESULT_DIR&quot; ]] &amp;&amp; mkdir -p &quot;$BASEDIR/$PROBE_RESULT_DIR&quot;; }
        local CHECK_STATUS BODY ERROR
        for ((i=1;i&lt;=&quot;$RETRY&quot;;i++)); do
          rm -f &quot;$RESP_BODY_FILE&quot; &quot;$ERROR_FILE&quot;
          CHECK_STATUS=&quot;$(curl -ksSL -w &#39;%{http_code}&#39; -m &quot;$TIMEOUT&quot; &quot;$URL&quot; -o &quot;$RESP_BODY_FILE&quot; 2&gt;&quot;$ERROR_FILE&quot;)&quot;
          [[ &quot;$CHECK_STATUS&quot; == 20* ]] &amp;&amp; echo &quot;$FUNC ok!&quot; &amp;&amp; {
            if [[ ! -z &quot;$SLEEP_WHEN_SUCCEED&quot; ]]; then
              echo &quot;sleep &quot;$SLEEP_WHEN_SUCCEED&quot;s before &quot;$FUNC&quot; return ...&quot; &amp;&amp; sleep &quot;$SLEEP_WHEN_SUCCEED&quot; 
            fi
            return 0
          } 
          ((&quot;$i&quot;&lt;&quot;$RETRY&quot;)) &amp;&amp; sleep &quot;$SLEEP&quot;
        done
        echo &quot;$FUNC failed after $RETRY times retry ...&quot;
        [[ -f &quot;$RESP_BODY_FILE&quot; ]] &amp;&amp; {
          BODY=&quot;$(cat &quot;$RESP_BODY_FILE&quot; | uniq)&quot;; [[ ! -z &quot;$BODY&quot; ]] &amp;&amp; echo &quot;http code is: $CHECK_STATUS; response body is: $BODY&quot;
        }
        [[ -f &quot;$ERROR_FILE&quot; ]] &amp;&amp; {
          ERROR=&quot;$(cat &quot;$ERROR_FILE&quot; | uniq)&quot;; [[ ! -z &quot;$ERROR&quot; ]] &amp;&amp; echo &quot;error is: $ERROR&quot;
        }
        return 1
      }
    cloudnative.com/offline-once.sh: |
      . &quot;$(cd &quot;$(dirname &quot;$0&quot;)&quot;;pwd)/http-probe.sh&quot;
      offline(){
        local URL=&quot;http://localhost:8888/health/offline&quot;
        local RETRY=1
        local SLEEP=10
        local TIMEOUT=3
        local APPHOME=/home/appops
        local SlEEP_SECONDS_WHEN_SUCCEED=0
        http_probe &quot;offline(下线)&quot; &quot;$URL&quot; &quot;$RETRY&quot; &quot;$SLEEP&quot; &quot;$TIMEOUT&quot; &quot;$SlEEP_SECONDS_WHEN_SUCCEED&quot; &quot;$APPHOME&quot;
      } &amp;&amp; offline
    cloudnative.com/offline.sh: |
      . &quot;$(cd &quot;$(dirname &quot;$0&quot;)&quot;;pwd)/http-probe.sh&quot;
      offline(){
        local URL=&quot;http://localhost:8888/health/offline&quot;
        local RETRY=2
        local SLEEP=10
        local TIMEOUT=5
        local APPHOME=/home/appops
        local SlEEP_SECONDS_WHEN_SUCCEED=10
        http_probe &quot;offline(下线)&quot; &quot;$URL&quot; &quot;$RETRY&quot; &quot;$SLEEP&quot; &quot;$TIMEOUT&quot; &quot;$SlEEP_SECONDS_WHEN_SUCCEED&quot; &quot;$APPHOME&quot;
      } &amp;&amp; offline
    cloudnative.com/online-once.sh: |
      . &quot;$(cd &quot;$(dirname &quot;$0&quot;)&quot;;pwd)/http-probe.sh&quot;
      online(){
        local URL=&quot;http://localhost:8888/health/active&quot;
        local RETRY=1
        local SLEEP=10
        local TIMEOUT=3
        local APPHOME=/home/appops
        local SlEEP_SECONDS_WHEN_SUCCEED=0
        http_probe &quot;online(上线)&quot; &quot;$URL&quot; &quot;$RETRY&quot; &quot;$SLEEP&quot; &quot;$TIMEOUT&quot; &quot;$SlEEP_SECONDS_WHEN_SUCCEED&quot; &quot;$APPHOME&quot;
      } &amp;&amp; online
    cloudnative.com/online.sh: |
      . &quot;$(cd &quot;$(dirname &quot;$0&quot;)&quot;;pwd)/http-probe.sh&quot;
      online(){
        local URL=&quot;http://localhost:8888/health/active&quot;
        local RETRY=50
        local SLEEP=10
        local TIMEOUT=30
        local APPHOME=/home/appops
        local SlEEP_SECONDS_WHEN_SUCCEED=0
        http_probe &quot;online(上线)&quot; &quot;$URL&quot; &quot;$RETRY&quot; &quot;$SLEEP&quot; &quot;$TIMEOUT&quot; &quot;$SlEEP_SECONDS_WHEN_SUCCEED&quot; &quot;$APPHOME&quot;
      }
      check(){
        local URL=&quot;http://localhost:8888/api/test&quot;
        local RETRY=50
        local SLEEP=10
        local TIMEOUT=30
        local APPHOME=/home/appops
        local SlEEP_SECONDS_WHEN_SUCCEED=0
        http_probe &quot;check(存活状态)&quot; &quot;$URL&quot; &quot;$RETRY&quot; &quot;$SLEEP&quot; &quot;$TIMEOUT&quot; &quot;$SlEEP_SECONDS_WHEN_SUCCEED&quot; &quot;$APPHOME&quot;
      }
      check &amp;&amp; online
    cloudnative.com/startup.sh: &gt;
      HTTP_CODE=&quot;$(curl -ksSL -w &#39;%{http_code}&#39; -m 3 -o /dev/null
      &quot;http://localhost:8888/api/test&quot;)&quot;

      ((&quot;$HTTP_CODE&quot;&lt;200)) || ((&quot;$HTTP_CODE&quot;&gt;=400)) &amp;&amp; exit 1

      exit 0
    cloudnative.com/status.sh: &gt;

      HTTP_CODE=&quot;$(curl -ksSL -w &#39;%{http_code}&#39; -m 3 -o /dev/null
      &quot;http://localhost:8888/api/test&quot;)&quot;

      ((&quot;$HTTP_CODE&quot;&lt;200)) || ((&quot;$HTTP_CODE&quot;&gt;=400)) &amp;&amp; exit 1

      is_json(){
        local INPUT=&quot;$1&quot;
        [[ ! -z &quot;$(jq -c &quot;objects&quot;&lt;&lt;&lt;&quot;$INPUT&quot; 2&gt;/dev/null)&quot; ]] &amp;&amp; return 0 || return 1
      }

      IFS=&#39;#&#39; read -r RESPONSE_BODY HTTP_CODE &lt; &lt;(curl -m 3 -s -w
      &#39;#%{http_code}&#39; &#39;http://localhost:8888/health/status&#39;)

      ((&quot;$HTTP_CODE&quot;&lt;200)) || ((&quot;$HTTP_CODE&quot;&gt;=400)) &amp;&amp; exit 1

      [[ -z &quot;$RESPONSE_BODY&quot; ]] &amp;&amp; exit 0

      RESPONSE_BODY=&quot;${RESPONSE_BODY,,}&quot;

      if [[ &quot;$RESPONSE_BODY&quot; != &quot;200&quot; ]] &amp;&amp; [[ &quot;$RESPONSE_BODY&quot; != &quot;alive&quot; ]] &amp;&amp;
      [[ &quot;$RESPONSE_BODY&quot; != &quot;ok&quot; ]] &amp;&amp; [[ &quot;$RESPONSE_BODY&quot; != &quot;online&quot; ]]; then
        if ! is_json &quot;$RESPONSE_BODY&quot;; then
          exit 1
        fi
        CODE=&quot;$(jq -r &#39;.code&#39;&lt;&lt;&lt;&quot;$RESPONSE_BODY&quot;)&quot;
        [[ &quot;$CODE&quot; == &quot;200&quot; ]] || exit 1
      fi</code></pre>
<p>可以看到，annotations里有很多的脚本，内容非常多，直接导致reload时要做的计算量非常的大</p>
<p>解决方式：<br>官方社区没有对此有任何的配置。只能内部魔改，将annotation从target的元数据里去掉，核心修改如下：</p>
<ol>
<li><a href="https://github.com/ryanwuer/prometheus/commit/8d345798884a2cab445766b9f5884f0c870deec6" target="_blank" rel="noopener">https://github.com/ryanwuer/prometheus/commit/8d345798884a2cab445766b9f5884f0c870deec6</a> —— 去掉 Pod 的 Annotation 和 label-present</li>
<li><a href="https://github.com/ryanwuer/prometheus/commit/660e9419b354c2cdeddfa6ddc2253a42f44e467a" target="_blank" rel="noopener">https://github.com/ryanwuer/prometheus/commit/660e9419b354c2cdeddfa6ddc2253a42f44e467a</a> —— 去掉 Endpoint 自己的元数据信息</li>
</ol>
<p>考虑到我们内部都是通过</p>
<ul>
<li>ServiceMonitor —— 通过 Service 的 Label</li>
<li>PodMonitor —— 通过 Pod 的 Label </li>
</ul>
<p>做服务发现，上述改动不会影响服务发现，实测CPU和内存使用下降非常明显，算是一个比较好的解决方案</p>
<h1 id="问题二：联邦模式下，子Prometheus双副本部署，通过thanos-query暴露服务，中心Prometheus如何配置拉取地址"><a href="#问题二：联邦模式下，子Prometheus双副本部署，通过thanos-query暴露服务，中心Prometheus如何配置拉取地址" class="headerlink" title="问题二：联邦模式下，子Prometheus双副本部署，通过thanos-query暴露服务，中心Prometheus如何配置拉取地址"></a>问题二：联邦模式下，子Prometheus双副本部署，通过thanos-query暴露服务，中心Prometheus如何配置拉取地址</h1><h2 id="方式1"><a href="#方式1" class="headerlink" title="方式1:"></a>方式1:</h2><p>thanos-query 不提供 /federate 接口，需要做一层转化，如下项目可满足需求：<br><a href="https://github.com/snapp-incubator/thanos-federate-proxy" target="_blank" rel="noopener">https://github.com/snapp-incubator/thanos-federate-proxy</a></p>
<h2 id="方式2"><a href="#方式2" class="headerlink" title="方式2:"></a>方式2:</h2><ol>
<li>子Prometheus配置自己的负载均衡域名，流量随机转到一台实例</li>
<li>中心prometheus，配置target为负载均衡域名，并丢弃 prometheus 副本标签，让指标看起来来自同一个实例</li>
</ol>
<pre><code class="yaml">metric_relabel_configs:
- regex: prometheus_replica
  action: labeldrop</code></pre>
<h1 id="问题三：Prometheus-Pod-反复重新调度，Pod一直处于销毁和重建中"><a href="#问题三：Prometheus-Pod-反复重新调度，Pod一直处于销毁和重建中" class="headerlink" title="问题三：Prometheus Pod 反复重新调度，Pod一直处于销毁和重建中"></a>问题三：Prometheus Pod 反复重新调度，Pod一直处于销毁和重建中</h1><p>我们通过 Prometheus-operator 交付 Prometheus Stack，operator会读取所有的PrometheusRule，并合并写到configmap中，并挂载给对应的 Prometheus Pod</p>
<pre><code class="yaml">- configMap:
  defaultMode: 420
  name: prometheus-kube-prometheus-kube-prome-prometheus-rulefiles-0</code></pre>
<p>在合并的过程中，configmap的大小可能会超过 1M，而configmap的大小限制是 1M，超过后会生成新的configmap，挂载spec的变化导致 Prometheus Pod 重新调度</p>
<blockquote>
<p>可是为什么会反复重新调度呢？</p>
</blockquote>
<p>平台有巡检服务的逻辑，会定时创建新服务，每个服务都会下发自己的PrometheusRule，发布成功后，会销毁掉这个服务，PrometheusRule也会销毁，在某些场景下，到了文件分割的临界点，巡检服务的PrometheusRule的存在与否，直接导致configmap的数量+1和-1，最终导致Pod 反复重新调度</p>
<h1 id="问题四：CPU-利用率震荡问题分析"><a href="#问题四：CPU-利用率震荡问题分析" class="headerlink" title="问题四：CPU 利用率震荡问题分析"></a>问题四：CPU 利用率震荡问题分析</h1><p>涉及Rate计算逻辑、时间戳设置逻辑，参见文章：<a href="https://ryanwuer.github.io/2024/07/17/prometheus-cpu-fluctuation/" target="_blank" rel="noopener">https://ryanwuer.github.io/2024/07/17/prometheus-cpu-fluctuation/</a></p>
<h1 id="问题五：查询慢"><a href="#问题五：查询慢" class="headerlink" title="问题五：查询慢"></a>问题五：查询慢</h1><p>老生常谈的高基数问题，基数可以理解为所有 Label 对可能的组合数，Prometheus的查询性能和数据的基数有关系，基数越高，写入和查询性能越差<br>我们用到的 CD组件：Argo-rollout（版本：v0.9.3）组件就存在这样的问题，通过指标裁剪，不再返回已经删除的argo-rollout的对应的指标</p>
<p>关于 label 较为合理的基数，可参考 Grafana 的两篇文章：<br><a href="https://grafana.com/blog/2022/02/15/what-are-cardinality-spikes-and-why-do-they-matter/" target="_blank" rel="noopener">what-are-cardinality-spikes-and-why-do-they-matter</a><br><a href="https://grafana.com/docs/grafana/latest/observability/metrics/prometheus/high-cardinality-metrics/" target="_blank" rel="noopener">high-cardinality-metrics</a></p>

  </article>
  <footer class="f-cf">
    
      <a href="/2025/04/08/grafana-iframe/" class="link f-fl">⟵Grafana 嵌入其他平台的实践</a>
    
    
      <a href="/2024/08/28/ingress-nginx-hands-on/" class="link f-fr">云原生网关 —— Ingress-nginx⟶</a>
    
  </footer>
</section>


    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<div id="gitalk-container"></div>
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: '4e08eb89d32a7fa72ccd',
        clientSecret: 'a3d809fa86c720cc9b944a53e689ad1e9779b162',
        id: location.pathname,
        repo: 'ryanwuer.github.io',
        owner: 'ryanwuer',
        admin: ['ryanwuer']
    })
    gitalk.render('gitalk-container')
</script>
</div>
    <footer id="footer" class="f-cf">
</footer>
</div>
<a href="https://github.com/ryanwuer" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub">
    <svg width="80" height="80" viewBox="0 0 250 250"
         style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
<style>.github-corner:hover .octo-arm {
        animation: octocat-wave 560ms ease-in-out
    }

    @keyframes octocat-wave {
        0%, 100% {
            transform: rotate(0)
        }
        20%, 60% {
            transform: rotate(-25deg)
        }
        40%, 80% {
            transform: rotate(10deg)
        }
    }

    @media (max-width: 500px) {
        .github-corner:hover .octo-arm {
            animation: none
        }

        .github-corner .octo-arm {
            animation: octocat-wave 560ms ease-in-out
        }
    }</style>
</body>
</html>
